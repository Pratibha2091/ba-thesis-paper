% !TEX root = ../main.tex

\chapter{Brief introduction to time series analysis}
\label{chap:Brief introduction to time series analysis}

\section{Definitions}
\label{sec:Definitions}

Time series is a collection of data points collected at constant time intervals. These are analyzed to determine the long term trend so as to forecast the future or perform some other form of analysis.

It is time dependent. Along with an increasing or decreasing trend, most time series have some form of seasonality trends, i.e. variations specific to a particular time frame. For example, if you see the sales of a woolen jacket over time, you will invariably find higher sales in winter seasons.

\section{Stationary process}
\label{sec:Stationary process}

Stationary process is stochastic process whose joint probabilities don't change when shifted in time. A stationary process therefore has the property that the mean, variance and autocorrelation structure do not change over time.

Many time series analysis methods depend on stationarity property.

\section{AR model}
\label{sec:AR model}

Autoregressive (AR)~\autocite{tsa} model is a representation of a type of random process. The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term).

Contrary to the MA model defined in Section~\ref{sec:MA model}, the AR model is not always stationary as it may contain a unit root.

AR~($p$) model is $p$-th order autoregressive model and it is defined as:
\begin{equation*}
  X_t = c + \sum_{i=1}^p{\varphi_i X_{t_1}} + \epsilon_t
\end{equation*}

$c$ and $\varphi$ are model parameters, $X_t$ is time series value at time step $t$, and $\epsilon_t$ is white noise with 0 mean and constant variance $\sigma_{\epsilon}^2$.

\section{MA model}
\label{sec:MA model}
The moving-average model~\autocite{tsa} specifies that the output variable depends linearly on its own previous stochastic term and on a stochastic term (an imperfectly predictable term).

Contrary to the AR model in Section~\ref{sec:AR model}, the MA model is always stationary.

MA~($q$) model is $q$-th order moving-average model defined as:

\begin{equation*}
  X_t = \mu + \sum_{i=1}^q{\varphi_i \epsilon_{t-i}}
\end{equation*}

where $\mu$ is time series mean and other notation is consistent with previous chapter.

\section{ARMA model}
\label{sec:ARMA model}

Autoregressive–moving-average (ARMA)~\autocite{tsa} models provide a parsimonious description of a (weakly) stationary stochastic process in terms of two polynomials, one for the auto-regression and the second for the moving average

Given a time series of data $X_t$, the ARMA model is a tool for understanding and, perhaps, predicting future values in this series. The model consists of two parts, an autoregressive (AR) part and a moving average (MA) part. The model is usually then referred to as the ARMA~($p$,$q$) model where $p$ is the order of the autoregressive part and $q$ is the order of the moving average part:

\begin{equation*}
  X_t = c + \sum_{i=1}^p{\varphi_i X_{t_1}} + \epsilon_t + \sum_{i=1}^q{\varphi_i \epsilon_{t-i}}
\end{equation*}


\section{ARIMA model}
\label{sec:ARIMA model}

Autoregressive integrated moving average (ARIMA) model~\autocite{tsa} is a generalization of an autoregressive moving average (ARMA) model. They are applied in some cases where data show evidence of non-stationarity, where an initial differencing step (corresponding to the “integrated” part of the model) can be applied to reduce the non-stationarity.

Previously defined models work only on stationary time series. To get time
series to stationary we need to transform it. Simplest such transformation is
differentiating adjacent elements into obtaining new time series. ARIMA ($p$,
$d$, $q$) model can be viewed as ARMA ($p$,$q$) applied to $d$-times differentiated original series.
